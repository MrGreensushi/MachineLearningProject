{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7_rPHBS49qm"
   },
   "source": [
    "# Import Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31913,
     "status": "ok",
     "timestamp": 1677682484124,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "FqMihrRrsdB8",
    "outputId": "6119ac87-b7c8-440c-859d-f914c50dcea2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb_cpIAnvYt9"
   },
   "outputs": [],
   "source": [
    "#!unzip \"/content/drive/MyDrive/paris.zip\" -d \"/content/drive/MyDrive/Paris120x120/\"  > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oziy1UvW7UPM"
   },
   "source": [
    "# Carica DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "computerDi=\"Daniele\"\n",
    "\n",
    "if(computerDi==\"Daniele\"):\n",
    "    groundtruth_dir = r\"C:\\Users\\dansp\\OneDrive\\Desktop\\gdz\"\n",
    "    dataset_dir = r\"C:\\Users\\dansp\\OneDrive\\Desktop\\paris\"\n",
    "elif(computerDi==\"Andrea\"):\n",
    "    groundtruth_dir = r\"D:\\Andrea\\Downloads\\gzp\"\n",
    "    dataset_dir = r\"D:\\Andrea\\Downloads\\Paris120x120\"\n",
    "else:\n",
    "    groundtruth_dir = r\"D:\\Andrea\\Downloads\\gzp\"\n",
    "    dataset_dir = r\"D:\\Andrea\\Downloads\\Paris120x120\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'load_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mload_dataset\u001b[39;00m\n\u001b[0;32m      3\u001b[0m class_names \u001b[38;5;241m=\u001b[39m load_dataset\u001b[38;5;241m.\u001b[39mget_classnames()\n\u001b[0;32m      4\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_names)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'load_dataset'"
     ]
    }
   ],
   "source": [
    "import load_dataset\n",
    "\n",
    "class_names = load_dataset.get_classnames()\n",
    "num_classes = len(class_names)\n",
    "\n",
    "dataset = load_dataset.get_dataset(groundtruth_dir, dataset_dir, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni per la stampa di statistiche sul DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1677682632194,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "EvuQ4lsS4B9O",
    "outputId": "0fb8851e-0a18-45d5-d0a5-6374f016dba9"
   },
   "outputs": [],
   "source": [
    "import print_functions as prt\n",
    "\n",
    "prt.print_dataset_stats(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "663Kn89ArM2C"
   },
   "source": [
    "# Split dei dati in training, validation, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1677682632195,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "-lE3T_GsjbR8",
    "outputId": "59f96407-6767-4949-e74f-25180b24cb98"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m       x_test[i][j] \u001b[38;5;241m=\u001b[39m x_test[i][j] \u001b[38;5;241m+\u001b[39m dataset[i][j][\u001b[38;5;241m-\u001b[39mtest_len:]\n\u001b[0;32m     24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m;\n\u001b[1;32m---> 26\u001b[0m x_train \u001b[38;5;241m=\u001b[39m [ [[],[],[],[]] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mclass_names\u001b[49m))]\n\u001b[0;32m     27\u001b[0m x_val \u001b[38;5;241m=\u001b[39m [ [[],[],[],[]] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(class_names))]\n\u001b[0;32m     28\u001b[0m x_test \u001b[38;5;241m=\u001b[39m [ [[],[],[],[]] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(class_names))]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "# Da Dataset -> A X_train, X_val, X_test\n",
    "\n",
    "# per avere uno split equilibrato, esso avverrà secondo il seguente criterio:\n",
    "# il training set sarà formato, per ogni landmark, dall' 80% del totale delle\n",
    "# good per quel landmark + 80% del totale delle ok per quel landmark + ecc...\n",
    "# il test set e il validation set avranno il 10% a testa\n",
    "\n",
    "def split_dataset(dataset, x_train, x_val, x_test):\n",
    "\n",
    "  for i in range(len(dataset)):\n",
    "    for j in range(len(dataset[i])): \n",
    "      \n",
    "      num_samples = len(dataset[i][j])\n",
    "\n",
    "      train_len = round(0.8 * num_samples)\n",
    "      x_train[i][j] = x_train[i][j] + dataset[i][j][:train_len]\n",
    "      \n",
    "      val_len = (num_samples - train_len) // 2\n",
    "      x_val[i][j] = x_val[i][j] + dataset[i][j][train_len: train_len + val_len]\n",
    "\n",
    "      test_len = (num_samples - train_len - val_len)\n",
    "      x_test[i][j] = x_test[i][j] + dataset[i][j][-test_len:]\n",
    "\n",
    "  return;\n",
    "\n",
    "x_train = [ [[],[],[],[]] for x in range(len(class_names))]\n",
    "x_val = [ [[],[],[],[]] for x in range(len(class_names))]\n",
    "x_test = [ [[],[],[],[]] for x in range(len(class_names))]\n",
    "\n",
    "split_dataset(dataset, x_train, x_val, x_test)\n",
    "\n",
    "x_train_len = prt.dataset_len(x_train)\n",
    "x_val_len = prt.dataset_len(x_val)\n",
    "\n",
    "print(prt.dataset_len(x_train))\n",
    "print(prt.dataset_len(x_val))\n",
    "print(prt.dataset_len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6rg72JV4uVX"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione degli ImageDataGenerator. Gli oggetti ImageDataGenerator definiscono le trasformazioni che si faranno.\n",
    "Chiamando flow(numpy_array_immagini) su ImageDataGenerator, flow restituirà un generatore di immagini transformate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oggetto ImageDataGenerator, definisce le trasformazioni\n",
    "gen_settings = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=60,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,  \n",
    "    vertical_flip=False,                     \n",
    "    rescale=None,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "#ImageDataGenerator senza trasformazioni per validation set\n",
    "test_val_settings = keras.preprocessing.image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora definiamo una classe custom che prende gli ImageDataGenerator del nostro dataset, e gli usa per creare un generatore di triplette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(object):\n",
    "    \n",
    "    # parametro nel costruttore: dataset_generators. Ha la solita struttura [ [[good],[ok],[bad],[junk]],  [...], ...]\n",
    "    # però invece che avere le foto nei good, ok, ... ho un generatore per quella categoria che genera le trasformazioni\n",
    "    \n",
    "    def __init__(self, dataset_generators,batch_size) -> None:\n",
    "        self.dataset_generators = dataset_generators\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "    def next_train(self):\n",
    "        current_class = -1\n",
    "        while 1:\n",
    "            anchors=[]\n",
    "            positives=[]\n",
    "            negatives=[]\n",
    "            for i in range(self.batch_size):\n",
    "                current_class = current_class+1\n",
    "                current_class = current_class % num_classes\n",
    "                \n",
    "                _anchor,_positive,_negative=retrieve_single_triplet(self,current_class)\n",
    "                anchors.append(_anchor)#se metto anchor invece di [anchor], quando faccio fit non funziona. Da capire\n",
    "                positives.append(_positive)\n",
    "                negatives.append(_negative)\n",
    "            \n",
    "            anchors=tf.convert_to_tensor(anchors)\n",
    "            positives=tf.convert_to_tensor(positives)\n",
    "            negatives=tf.convert_to_tensor(negatives)\n",
    "            \n",
    "            triplet = (anchors, positives, negatives)\n",
    "            yield(triplet)\n",
    "        \n",
    "def retrieve_single_triplet(self,current_class):\n",
    "    different_class = random.randint(0, num_classes-1)\n",
    "    \n",
    "    while(different_class == current_class):\n",
    "        different_class = random.randint(0, num_classes-1)  \n",
    "            \n",
    "    _anchor = next(self.dataset_generators[current_class][1])[0]\n",
    "    _positive = next(self.dataset_generators[current_class][1])[0]\n",
    "    _negative = next(self.dataset_generators[different_class][random.choice([1,3])])[0]    \n",
    "    return _anchor,_positive,_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train, x_val, x_test sono strutture dati di immagini così disposte: [ [[good],[ok],[bad],[junk]],  [...], ...]\n",
    "Sostituiamo le immagini coi generatori creati a partire da quelle immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#x_train, x_val, x_test prima contenevano array di immagini. Ora contengono generatori separati per classe e qualità\n",
    "def transform_dataset(ds, gen_settings):\n",
    "    for i in range(len(ds)):\n",
    "        for j in [1,3]: #abbiamo solo immagini ok e junk (1 e 3) nel database\n",
    "            generatore = gen_settings.flow(x=np.asarray(ds[i][j]), batch_size=1) #batch size 1 per ritornare una sola img\n",
    "            ds[i][j] = generatore\n",
    "    return ds\n",
    "\n",
    "x_train_gen = transform_dataset(x_train, gen_settings)\n",
    "x_val_gen = transform_dataset(x_val, test_val_settings)\n",
    "#x_test = transform_dataset(x_test, test_val_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "#Training generator\n",
    "custom_gen_train = CustomDataGenerator(x_train_gen,batch_size)\n",
    "generatore_triplete_train = custom_gen_train.next_train()\n",
    "\n",
    "#Validation generator\n",
    "custom_gen_val = CustomDataGenerator(x_val_gen,batch_size)\n",
    "generatore_triplete_val = custom_gen_val.next_train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prt.stampa_tripletta(next(generatore_triplete_train))\n",
    "prt.stampa_tripletta(next(generatore_triplete_train))\n",
    "prt.stampa_tripletta(next(generatore_triplete_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTfKkXSex5hY"
   },
   "source": [
    "# Definizione Modello Triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1677683208972,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "jPhBXKjtx_Ez",
    "outputId": "d5fd85c2-6080-4518-c49b-1a527036a068"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow.keras.applications.resnet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4961,
     "status": "ok",
     "timestamp": 1677683214164,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "LPb1dxnQlpKw",
    "outputId": "0eec6b29-94d6-447a-e338-664ebac8dbff"
   },
   "outputs": [],
   "source": [
    "def get_encoder (input_shape):\n",
    "    \n",
    "    base_cnn = resnet.ResNet50(\n",
    "    weights=\"imagenet\", input_shape=input_shape, include_top=False)\n",
    "    \n",
    "    trainable = False\n",
    "    for layer in base_cnn.layers:\n",
    "        if layer.name == \"conv5_block1_out\":\n",
    "            trainable = True\n",
    "        layer.trainable = trainable\n",
    "        \n",
    "    encode_model = tf.keras.models.Sequential([\n",
    "        base_cnn,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(32)\n",
    "    ], name=\"Embedding\")\n",
    "    \n",
    "    \n",
    "    return encode_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "def get_siamese_network (input_shape = (120,120,3)):\n",
    "    embedding_model = get_encoder(input_shape)\n",
    "    input_anchor = tf.keras.layers.Input(shape= input_shape)\n",
    "    input_positive = tf.keras.layers.Input(shape= input_shape)\n",
    "    input_negative = tf.keras.layers.Input(shape= input_shape)\n",
    "    \n",
    "    embedding_anchor = embedding_model(resnet.preprocess_input(input_anchor))\n",
    "    embedding_positive = embedding_model(resnet.preprocess_input(input_positive))\n",
    "    embedding_negative = embedding_model(resnet.preprocess_input(input_negative))\n",
    "\n",
    "    #embedding_anchor = embedding_model(input_anchor)\n",
    "    #embedding_positive = embedding_model(input_positive)\n",
    "    #embedding_negative = embedding_model(input_negative)\n",
    "\n",
    "    distances = DistanceLayer()(\n",
    "    embedding_anchor ,\n",
    "    embedding_positive,\n",
    "    embedding_negative ,\n",
    "    )\n",
    "\n",
    "    siamese_network =  tf.keras.models.Model(\n",
    "    inputs=[input_anchor, input_positive, input_negative], outputs=distances\n",
    "    )\n",
    "    return siamese_network\n",
    "    \n",
    "siamese_network = get_siamese_network()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1677683216640,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "QiNCCICl-TSs"
   },
   "outputs": [],
   "source": [
    "class SiameseModel(tf.keras.models.Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker =tf.keras.metrics.BinaryAccuracy(name =\"accuracy\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss,acc = self._compute_loss(data)\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state((acc|True),acc)\n",
    "        return {\"loss\": self.loss_tracker.result(),\"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \n",
    "        loss,acc = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state((acc|True),acc)\n",
    "        return {\"loss\": self.loss_tracker.result(),\"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss, (ap_distance<an_distance)\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker,self.acc_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), weighted_metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3217700,
     "status": "ok",
     "timestamp": 1677686434335,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "PQeFNZa3-fJQ",
    "outputId": "d12c0e71-9968-4c0c-dee9-9f57da6683ad",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#history = siamese_model.fit(triplet_train, epochs=1, validation_data=triplet_val)\n",
    "\n",
    "# siccome uso un generator, la dimensione del dataset diventa potenzialmente infinita\n",
    "# devo definire a quanti batch corrisponde un'epoca -> steps per epoch, validation steps\n",
    "steps_train=int( x_train_len/batch_size )\n",
    "steps_val=int(x_val_len/batch_size)\n",
    "\n",
    "history = siamese_model.fit(\n",
    "    generatore_triplete_train,\n",
    "    steps_per_epoch=steps_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data=generatore_triplete_val,\n",
    "    validation_steps=steps_val,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 5962,
     "status": "ok",
     "timestamp": 1677686440291,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "3-V2JF8uwJjE",
    "outputId": "ef42d28c-f71b-4cc0-b6ca-e3ac7a092d78"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy & loss')\n",
    "plt.ylabel('accuracy & loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['acc train', 'acc test', 'loss train','loss test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usare il modello allenato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnare questa cella solo se non si ha il modello \"encoder\" salvato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1677686440293,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "zq3i8cK5TL-d",
    "outputId": "39cab78b-8789-49ff-8c6f-6eda0fe21c8f"
   },
   "outputs": [],
   "source": [
    "def exctract_embedding(model):\n",
    "    encoder=get_encoder((120,120,3))\n",
    "    i=0\n",
    "    for e_layer in model.layers[0].layers[9].layers:\n",
    "        layer_weight= e_layer.get_weights()\n",
    "        encoder.layers[i].set_weights(layer_weight)\n",
    "        i+=1\n",
    "    return encoder\n",
    "encoder=exctract_embedding(siamese_model)\n",
    "encoder.save_weights(\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGljr94hSLhu",
    "outputId": "3850ea9e-1154-45e8-a58e-3c7efb68c0f0"
   },
   "source": [
    "Runnare questa cella solo se si sono già calcolati i pesi per il modello \"encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=get_encoder((120,120,3))\n",
    "encoder.load_weights(\"encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data un immagine in ingresso, trovare quale immagine appartenente al training set sia la più vicina, e classificare l'immagine in base al graound truth dell'immagine più simile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcolo per ogni immagine del training set l'output attraverso l'encode\n",
    "def makeX_Y_array (array):\n",
    "    arr=[] #array 'senza buchi' \n",
    "    y_train= [] #array contenente la classe in base all'immagine\n",
    "    y_levels=[] \n",
    "    previous=0\n",
    "    for index in range(len(class_names)):\n",
    "        arr.extend(array[index][0]) # [i] indica la classe [j] indica la qualita [k] la foto singola\n",
    "        arr.extend(array[index][1]) #creo dataset di una sola classe con immagini solo good e ok\n",
    "        totLen=len(array[index][0])+len(array[index][1])\n",
    "       \n",
    "        k= np.zeros(len(class_names))\n",
    "        k[index]=1.0\n",
    "        for el in range(totLen):\n",
    "            y_train.append(k)\n",
    "            \n",
    "        y_levels.append(previous+totLen)\n",
    "        previous=y_levels[index] # y_levels indica il valore più alto appartenente alla classe indicata da index\n",
    "        \n",
    "    dt= tf.data.Dataset.from_tensor_slices(arr)\n",
    "    y_train = tf.data.Dataset.from_tensor_slices (y_train)\n",
    "    dt=dt.batch(32,drop_remainder=False)\n",
    "    y_train=y_train.batch(32,drop_remainder=False)\n",
    "    return dt, y_train,y_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt,y_train,y_levels = makeX_Y_array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points=encoder.predict(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(tensor):\n",
    "    distance = tf.reduce_sum(tf.square(points - tensor), -1)\n",
    "    return distance\n",
    "\n",
    "def retrieve_mean_distance(tensor):\n",
    "    distance = []    \n",
    "    for index in range(len(class_names)):\n",
    "        if index==0:\n",
    "            diClasse=tensor[0:y_levels[index]]\n",
    "        else:\n",
    "            diClasse=tensor[y_levels[index-1]:y_levels[index]]\n",
    "          \n",
    "        distance.append( tf.math.reduce_min(diClasse) )  # per ritrovare la media basta mettere reduce_mean\n",
    "    #toRet= tf.reduce_min(distance) \n",
    "    #distribution = tf.math.equal(distance,toRet)\n",
    "    #distribution = tf.cast(distribution,tf.float32)\n",
    "    distribution = tf.nn.softmax(distance)\n",
    "    distribution =tf.subtract(distribution,1)\n",
    "    distribution = tf.negative(distribution)\n",
    "    distribution= tf.divide(distribution,len(distribution)-1)\n",
    "    return distribution #l'indice della classe con la distanza media minore\n",
    "\n",
    "class DistanceClassificationLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.points=points\n",
    "\n",
    "    def call(self, tensor):\n",
    "        #per ogni immagine vado a calcolare la distanza rispetto a tutti le foto del training set\n",
    "        predicts = tf.map_fn(classify_images,tensor)\n",
    "        #PER USARE LA MEDIA INVECE CHE IL MINIMO SCOMMENTRARE QUESTI 2 E COMMENTARE I 2 SUCCESSIVI\n",
    "        test=tf.map_fn(retrieve_mean_distance,predicts)\n",
    "        return test\n",
    "        #predicts=tf.argmin(predicts,-1) #prendo l'indice dell'immagine rispetto al training set con la distanza minore\n",
    "        #return retrieve_label(predicts) # ritrovo dall'indice la classe\n",
    "    \n",
    "       \n",
    "evaluation_model=tf.keras.models.Sequential([encoder,DistanceClassificationLayer()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vado a creare un dataset dove (x_test, classi associata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dt,res,re_levels= makeX_Y_array(x_test)\n",
    "evaluate_dt= tf.data.Dataset.zip((evaluate_dt,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_model.compile(metrics=[tf.keras.metrics.CategoricalAccuracy()]) # qui ci andrebbe il topK ma non funziona\n",
    "evaluation_model.compile(metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=3)]) # qui ci andrebbe il topK ma non funziona\n",
    "evaluation_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_model.evaluate(evaluate_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)\n",
    "print(len(evaluate_dt), len(evaluate_dt[0]), len(evaluate_dt[0][0]), len(evaluate_dt[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = evaluation_model.predict(evaluate_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "Oziy1UvW7UPM",
    "YrGZaLez8Mzp",
    "663Kn89ArM2C",
    "RO9_mdQNg30A",
    "MwJhSXsz42Jf"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
