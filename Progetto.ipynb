{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7_rPHBS49qm"
   },
   "source": [
    "# Import Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31913,
     "status": "ok",
     "timestamp": 1677682484124,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "FqMihrRrsdB8",
    "outputId": "6119ac87-b7c8-440c-859d-f914c50dcea2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb_cpIAnvYt9"
   },
   "outputs": [],
   "source": [
    "#!unzip \"/content/drive/MyDrive/paris.zip\" -d \"/content/drive/MyDrive/Paris120x120/\"  > /dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Oziy1UvW7UPM"
   },
   "source": [
    "# Carica DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_dataset\n",
    "user=\"Daniele\"\n",
    "\n",
    "load_dataset.set_user(user)\n",
    "groundtruth_dir, dataset_dir = load_dataset.get_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = load_dataset.get_classnames()\n",
    "num_classes = len(class_names)\n",
    "\n",
    "dataset = load_dataset.get_dataset(groundtruth_dir, dataset_dir, class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni per la stampa di statistiche sul DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1677682632194,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "EvuQ4lsS4B9O",
    "outputId": "0fb8851e-0a18-45d5-d0a5-6374f016dba9"
   },
   "outputs": [],
   "source": [
    "import print_functions as prt\n",
    "\n",
    "prt.print_dataset_stats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prt.print_class_good_images(dataset,0) #stampa tutte le immagini good della classe 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "663Kn89ArM2C"
   },
   "source": [
    "# Split dei dati in training, validation, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_dataset.shuffle(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1677682632195,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "-lE3T_GsjbR8",
    "outputId": "59f96407-6767-4949-e74f-25180b24cb98"
   },
   "outputs": [],
   "source": [
    "# Da Dataset -> A X_train, X_val, X_test\n",
    "\n",
    "# per avere uno split equilibrato, esso avverrà secondo il seguente criterio:\n",
    "# il training set sarà formato, per ogni landmark, dall' 80% del totale delle\n",
    "# good per quel landmark + 80% del totale delle ok per quel landmark + ecc...\n",
    "# il test set e il validation set avranno il 10% a testa\n",
    "\n",
    "def split_dataset(dataset, x_train, x_val, x_test):\n",
    "\n",
    "  for i in range(len(dataset)):\n",
    "    for j in range(len(dataset[i])): \n",
    "      \n",
    "      num_samples = len(dataset[i][j])\n",
    "\n",
    "      train_len = round(0.8 * num_samples)\n",
    "      x_train[i][j] = x_train[i][j] + dataset[i][j][:train_len]\n",
    "      \n",
    "      val_len = (num_samples - train_len) // 2\n",
    "      x_val[i][j] = x_val[i][j] + dataset[i][j][train_len: train_len + val_len]\n",
    "\n",
    "      test_len = (num_samples - train_len - val_len)\n",
    "      x_test[i][j] = x_test[i][j] + dataset[i][j][-test_len:]\n",
    "\n",
    "  return;\n",
    "\n",
    "x_train = [ [[],[],[],[]] for x in range(len(class_names))]\n",
    "x_val = [ [[],[],[],[]] for x in range(len(class_names))]\n",
    "x_test = [ [[],[],[],[]] for x in range(len(class_names))]\n",
    "\n",
    "split_dataset(dataset, x_train, x_val, x_test)\n",
    "\n",
    "x_train_len = prt.dataset_len(x_train)\n",
    "x_val_len = prt.dataset_len(x_val)\n",
    "\n",
    "print(prt.dataset_len(x_train))\n",
    "print(prt.dataset_len(x_val))\n",
    "print(prt.dataset_len(x_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F6rg72JV4uVX"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione degli ImageDataGenerator. Gli oggetti ImageDataGenerator definiscono le trasformazioni che si faranno.\n",
    "Chiamando flow(numpy_array_immagini) su ImageDataGenerator, flow restituirà un generatore di immagini transformate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oggetto ImageDataGenerator, definisce le trasformazioni\n",
    "gen_settings = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=60,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,  \n",
    "    vertical_flip=False,                     \n",
    "    rescale=None,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "#ImageDataGenerator senza trasformazioni per validation set\n",
    "test_val_settings = keras.preprocessing.image.ImageDataGenerator()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora definiamo una classe custom che prende gli ImageDataGenerator del nostro dataset, e gli usa per creare un generatore di triplette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(object):\n",
    "    \n",
    "    # parametro nel costruttore: dataset_generators. Ha la solita struttura [ [[good],[ok],[bad],[junk]],  [...], ...]\n",
    "    # però invece che avere le foto nei good, ok, ... ho un generatore per quella categoria che genera le trasformazioni\n",
    "    \n",
    "    def __init__(self, dataset_generators,batch_size) -> None:\n",
    "        self.dataset_generators = dataset_generators\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "    def next_train(self):\n",
    "        current_class = -1\n",
    "        while 1:\n",
    "            anchors=[]\n",
    "            positives=[]\n",
    "            negatives=[]\n",
    "            for i in range(self.batch_size):\n",
    "                current_class = current_class+1\n",
    "                current_class = current_class % num_classes\n",
    "                \n",
    "                _anchor,_positive,_negative = retrieve_single_triplet(self,current_class)\n",
    "                anchors.append(_anchor)\n",
    "                positives.append(_positive)\n",
    "                negatives.append(_negative)\n",
    "            \n",
    "            anchors = tf.convert_to_tensor(anchors)\n",
    "            positives = tf.convert_to_tensor(positives)\n",
    "            negatives = tf.convert_to_tensor(negatives)\n",
    "\n",
    "            triplet = (anchors, positives, negatives)\n",
    "            yield(triplet)\n",
    "        \n",
    "def retrieve_single_triplet(self,current_class):\n",
    "    different_class = random.randint(0, num_classes-1)\n",
    "    \n",
    "    while(different_class == current_class):\n",
    "        different_class = random.randint(0, num_classes-1)  \n",
    "            \n",
    "    _anchor = next(self.dataset_generators[current_class][1])[0]\n",
    "    _positive = next(self.dataset_generators[current_class][1])[0]\n",
    "    _negative = next(self.dataset_generators[different_class][random.choice([1,3])])[0]    \n",
    "    return _anchor,_positive,_negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train, x_val, x_test sono strutture dati di immagini così disposte: [ [[good],[ok],[bad],[junk]],  [...], ...]\n",
    "Sostituiamo le immagini coi generatori creati a partire da quelle immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#x_train, x_val, x_test prima contenevano array di immagini. Ora contengono generatori separati per classe e qualità\n",
    "def transform_dataset(db, gen_settings):\n",
    "    ds = [ [[],[],[],[]] for x in range(len(class_names))]\n",
    "    for i in range(len(ds)):\n",
    "        for j in [1,3]: #abbiamo solo immagini ok e junk (1 e 3) nel database\n",
    "            generatore = gen_settings.flow(x=np.asarray(db[i][j]), batch_size=1) #batch size 1 per ritornare una sola img\n",
    "            ds[i][j] = generatore\n",
    "    return ds\n",
    "\n",
    "x_train_gen = transform_dataset(x_train, gen_settings)\n",
    "x_val_gen = transform_dataset(x_val, test_val_settings)\n",
    "#x_test = transform_dataset(x_test, test_val_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "#Training generator\n",
    "custom_gen_train = CustomDataGenerator(x_train_gen,batch_size)\n",
    "generatore_triplete_train = custom_gen_train.next_train()\n",
    "\n",
    "#Validation generator\n",
    "custom_gen_val = CustomDataGenerator(x_val_gen,batch_size)\n",
    "generatore_triplete_val = custom_gen_val.next_train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplete_batch = next(generatore_triplete_train)\n",
    "\n",
    "ancore = triplete_batch[0]\n",
    "positivi = triplete_batch[1]\n",
    "negativi = triplete_batch[2]\n",
    "\n",
    "tripla1 = (ancore[0], positivi[0], negativi[0])\n",
    "tripla2 = (ancore[1], positivi[1], negativi[1])\n",
    "tripla3 = (ancore[2], positivi[2], negativi[2])\n",
    "\n",
    "prt.stampa_tripletta(tripla1)\n",
    "prt.stampa_tripletta(tripla2)\n",
    "prt.stampa_tripletta(tripla3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GTfKkXSex5hY"
   },
   "source": [
    "# Definizione Modello Triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1677683208972,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "jPhBXKjtx_Ez",
    "outputId": "d5fd85c2-6080-4518-c49b-1a527036a068"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "#from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4961,
     "status": "ok",
     "timestamp": 1677683214164,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "LPb1dxnQlpKw",
    "outputId": "0eec6b29-94d6-447a-e338-664ebac8dbff"
   },
   "outputs": [],
   "source": [
    "def get_encoder (input_shape):\n",
    "    \n",
    "    \n",
    "    #base_cnn = resnet.ResNet50(\n",
    "    #base_cnn = resnet.ResNet101(\n",
    "    #base_cnn = resnet.ResNet152(\n",
    "    base_cnn = EfficientNetB7(\n",
    "        weights=\"imagenet\", input_shape=input_shape, include_top=False)\n",
    "    \n",
    "    trainable = False\n",
    "    for layer in base_cnn.layers:\n",
    "        if layer.name == \"conv5_block1_out\":\n",
    "            trainable = True\n",
    "        layer.trainable = trainable\n",
    "        \n",
    "    encode_model = tf.keras.models.Sequential([\n",
    "        base_cnn,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(32)\n",
    "    ], name=\"Embedding\")\n",
    "    \n",
    "    \n",
    "    return encode_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2477,
     "status": "ok",
     "timestamp": 1677683216639,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "xLpYIIVB8umU"
   },
   "outputs": [],
   "source": [
    "def get_siamese_network (input_shape = (120,120,3)):\n",
    "    embedding_model = get_encoder(input_shape)\n",
    "    input_anchor = tf.keras.layers.Input(shape= input_shape)\n",
    "    input_positive = tf.keras.layers.Input(shape= input_shape)\n",
    "    input_negative = tf.keras.layers.Input(shape= input_shape)\n",
    "    \n",
    "    #embedding_anchor = embedding_model(resnet.preprocess_input(input_anchor))\n",
    "    #embedding_positive = embedding_model(resnet.preprocess_input(input_positive))\n",
    "    #embedding_negative = embedding_model(resnet.preprocess_input(input_negative))\n",
    "\n",
    "    embedding_anchor = embedding_model(input_anchor)\n",
    "    embedding_positive = embedding_model(input_positive)\n",
    "    embedding_negative = embedding_model(input_negative)\n",
    "\n",
    "    distances = DistanceLayer()(\n",
    "    embedding_anchor ,\n",
    "    embedding_positive,\n",
    "    embedding_negative ,\n",
    "    )\n",
    "\n",
    "    siamese_network =  tf.keras.models.Model(\n",
    "    inputs=[input_anchor, input_positive, input_negative], outputs=distances\n",
    "    )\n",
    "    return siamese_network\n",
    "    \n",
    "siamese_network = get_siamese_network()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1677683216640,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "QiNCCICl-TSs"
   },
   "outputs": [],
   "source": [
    "class SiameseModel(tf.keras.models.Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker =tf.keras.metrics.BinaryAccuracy(name =\"accuracy\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss,acc = self._compute_loss(data)\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state((acc|True),acc)\n",
    "        return {\"loss\": self.loss_tracker.result(),\"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \n",
    "        loss,acc = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state((acc|True),acc)\n",
    "        return {\"loss\": self.loss_tracker.result(),\"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss, (ap_distance<an_distance)\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker,self.acc_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.00100\n",
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(lr), weighted_metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3217700,
     "status": "ok",
     "timestamp": 1677686434335,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "PQeFNZa3-fJQ",
    "outputId": "d12c0e71-9968-4c0c-dee9-9f57da6683ad",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "PATIENCE_END=10\n",
    "PATIENCE_REDUCE=3\n",
    "COOLDOWN=3\n",
    "VERBOSE=1\n",
    "NAME=f\"EfficientNetB7_{lr}\"\n",
    "EPOCHS=100\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=PATIENCE_END,\n",
    "                                                    mode='min',\n",
    "                                                    restore_best_weights='val_acc')\n",
    "\n",
    "reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                               factor=0.5,\n",
    "                                               patience=PATIENCE_REDUCE,\n",
    "                                               verbose=VERBOSE,\n",
    "                                               mode='auto',\n",
    "                                               min_delta=0.01,\n",
    "                                               cooldown=COOLDOWN,\n",
    "                                               min_lr=1e-6\n",
    "                                              )\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/{NAME}\")\n",
    "\n",
    "# siccome uso un generator, la dimensione del dataset diventa potenzialmente infinita\n",
    "# devo definire a quanti batch corrisponde un'epoca -> steps per epoch, validation steps\n",
    "steps_train=int( x_train_len/batch_size )\n",
    "steps_val=int(x_val_len/batch_size)\n",
    "\n",
    "#callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=6, restore_best_weights='val_acc')\n",
    "#callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=30, restore_best_weights='val_acc')\n",
    "\n",
    "history = siamese_model.fit(\n",
    "    generatore_triplete_train,\n",
    "    steps_per_epoch=steps_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=generatore_triplete_val,\n",
    "    validation_steps=steps_val,\n",
    "    callbacks=[early_stopping,reduce_lr,tensorboard_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 5962,
     "status": "ok",
     "timestamp": 1677686440291,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "3-V2JF8uwJjE",
    "outputId": "ef42d28c-f71b-4cc0-b6ca-e3ac7a092d78"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy & loss')\n",
    "plt.ylabel('accuracy & loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['acc train', 'acc test', 'loss train','loss test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usare il Modello Allenato"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnare questa cella solo se non si ha il modello \"encoder\" salvato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1677686440293,
     "user": {
      "displayName": "Hathi “danspi” Branco Wild",
      "userId": "03173318165689254341"
     },
     "user_tz": -60
    },
    "id": "zq3i8cK5TL-d",
    "outputId": "39cab78b-8789-49ff-8c6f-6eda0fe21c8f"
   },
   "outputs": [],
   "source": [
    "def get_encoder_with_weights(model):\n",
    "    encoder=get_encoder((120,120,3))\n",
    "    i=0\n",
    "    for e_layer in model.layers[0].layers[3].layers:\n",
    "        layer_weight= e_layer.get_weights()\n",
    "        encoder.layers[i].set_weights(layer_weight)\n",
    "        i+=1\n",
    "    return encoder\n",
    "\n",
    "encoder = get_encoder_with_weights(siamese_model)\n",
    "#encoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGljr94hSLhu",
    "outputId": "3850ea9e-1154-45e8-a58e-3c7efb68c0f0"
   },
   "source": [
    "Runnare questa cella solo se si sono già calcolati i pesi per il modello \"encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder=get_encoder((120,120,3))\n",
    "#encoder.load_weights(\"PesiMigliori\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni per cambiare la rappresentazione da [[ [good], [ok], [bad], [junk] ],[...], ...] ad un vettore normale + vettore di label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prende tutte le immagini good e ok da data e le mette in un array arr, con le immagini ordinate per classe crescente\n",
    "def to_array(data):\n",
    "    arr=[]\n",
    "    for index in range(num_classes):\n",
    "        arr.extend(data[index][0]) #prendo le immagini good della classe index, e le appendo in arr\n",
    "        arr.extend(data[index][1]) #prendo le immagini ok della classe index, e le appendo in arr\n",
    "    print(\"Data transformed into array\")\n",
    "    return arr\n",
    "\n",
    "#array che segna dove finiscono in arr le immagini della classe i-esima\n",
    "def get_levels(data):\n",
    "    y_levels=[]\n",
    "    previous = 0\n",
    "    for index in range(num_classes):\n",
    "        i_class_len = len(data[index][0])+len(data[index][1])\n",
    "        y_levels.append(previous+i_class_len)\n",
    "        previous = previous + i_class_len\n",
    "    return y_levels\n",
    "\n",
    "#ritorna il vettore y di label(le classi) a partire da un y_level\n",
    "def get_y(y_levels):\n",
    "    tot = y_levels[-1]\n",
    "    y = [np.zeros(num_classes) for i in range(tot)]\n",
    "    class_index = 0\n",
    "    \n",
    "    for i in range(tot):\n",
    "        while i >= y_levels[class_index]:\n",
    "            class_index = class_index + 1\n",
    "        y[i][class_index] = 1\n",
    "    return y\n",
    "\n",
    "def produce_encodings(x, encoder):\n",
    "    dt = tf.data.Dataset.from_tensor_slices(x)\n",
    "    dt = dt.batch(1,drop_remainder=False)\n",
    "    print(\"Encoding data...\")\n",
    "    return encoder.predict(dt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo le immagini di training attraverso l'encoder e produco un encoding delle immagini di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_levels = get_levels(x_train)\n",
    "x_train_array = to_array(x_train)\n",
    "x_train_encoded = produce_encodings(x_train_array, encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo le immagini di test attraverso l'encoder e produco un encoding delle immagini di test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_levels = get_levels(x_test)\n",
    "x_test_array = to_array(x_test)\n",
    "x_test_encoded = produce_encodings(x_test_array, encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ogni immagine di test, ottengo un vettore con le probabilità che appartenga ad una certa classe (come il layer softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predizioni(x_train_encoded, x_test_encoded, y_train_levels):\n",
    "    predizioni = []\n",
    "    for i in range(len(x_test_encoded)):\n",
    "        distances = tf.reduce_sum(tf.square(x_train_encoded - x_test_encoded[i]), -1)\n",
    "\n",
    "        x = []  \n",
    "        for index in range(num_classes):\n",
    "            if index==0:\n",
    "                diClasse=distances[0:y_train_levels[index]]\n",
    "            else:\n",
    "                diClasse=distances[y_train_levels[index-1]:y_train_levels[index]]\n",
    "            x.append( tf.math.reduce_min(diClasse) )\n",
    "        distribution = tf.nn.softmax(x)\n",
    "        distribution = tf.negative(distribution)\n",
    "\n",
    "        predizioni.append(distribution.numpy().tolist())\n",
    "    return predizioni\n",
    "\n",
    "predizioni = get_predizioni(x_train_encoded, x_test_encoded, y_train_levels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo e stampa del topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def topk(num):\n",
    "    x = tf.keras.metrics.TopKCategoricalAccuracy(k=num)\n",
    "    x.update_state(get_y(y_test_levels),predizioni)\n",
    "    return x.result().numpy()\n",
    "\n",
    "k = [0,0,0,0] #top k=1,k=2,k=3,k=4\n",
    "\n",
    "for i in range(len(k)):\n",
    "    k[i] = topk(i+1)\n",
    "\n",
    "print(\"Valori del topk:\", k)\n",
    "print(\"Valore medio del topk:\", mean(k))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(FACOLTATIVO/NON SERVE)\n",
    "\n",
    "Printiamo una matrice artigianale di predizioni sul test set (ci dà un idea se siamo sulla buona strada). Una matrice diagonale è l'ideale (darebbe un top k=1 uguale a 1.0). Attenzione che se il modello predice le classi con la stessa identica probabilità, nella matrice metto come predizione la prima classe nell'ordine dei class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice = [[0 for i in range(len(predizioni[0]))] for i in range(num_classes)]\n",
    "\n",
    "o = 0\n",
    "for i in range(len(predizioni)):\n",
    "    if y_test_levels[o]-1 < i:\n",
    "        o = o+1\n",
    "    index = 0\n",
    "    m = predizioni[i][0]\n",
    "    for j in range(len(predizioni[i])):\n",
    "        if predizioni[i][j] > m:\n",
    "            m = predizioni[i][j]\n",
    "            index = j\n",
    "    matrice[o][index] = matrice[o][index] + 1\n",
    "        \n",
    "print(\"Le righe sono il ground truth. Cioè riga i = classe i\");\n",
    "print(\"Le colonne sono le predizioni.\\nUn buon lavoro restituisce una matrice con la diagonale piena e il resto vuoto\");\n",
    "\n",
    "for i in range(num_classes):\n",
    "    print(matrice[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvare i Pesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_encoder = get_encoder((120,120,3))\n",
    "old_encoder.load_weights(\"PesiMigliori\")\n",
    "\n",
    "x_train_encoded = produce_encodings(x_train_array, old_encoder)\n",
    "x_test_encoded = produce_encodings(x_test_array, old_encoder)\n",
    "\n",
    "predizioni = get_predizioni(x_train_encoded, x_test_encoded, y_train_levels)\n",
    "old_k = [0,0,0,0] #top k=1,k=2,k=3,k=4\n",
    "\n",
    "for i in range(len(old_k)):\n",
    "    old_k[i] = topk(i+1)\n",
    "\n",
    "print(\"Valori del topk:\", k)\n",
    "print(\"Valore medio del topk:\", mean(k))\n",
    "\n",
    "print(\"Valori del topk vecchio:\", old_k)\n",
    "print(\"Valore medio del topk vecchio:\", mean(old_k))\n",
    "\n",
    "if mean(k) > mean(old_k):\n",
    "    print(\"Nuovi pesi salvati\")\n",
    "    encoder.save_weights(\"PesiMigliori\")\n",
    "else:\n",
    "    print(\"Nuovi pesi NON salvati\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights(\"Pesi_Dell_Ultima_Run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "Oziy1UvW7UPM",
    "YrGZaLez8Mzp",
    "663Kn89ArM2C",
    "RO9_mdQNg30A",
    "MwJhSXsz42Jf"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
